{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt #Matplotlib Library\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#SkLearn Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "#Keras Libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Kernel is organized in the following topics:\n",
    "    * EDA\n",
    "    * Data cleaning\n",
    "    * Model Developing:\n",
    "        * Random Forest\n",
    "        * Light Gradient Boosting\n",
    "        * Neural Network\n",
    "    * Model comparison        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6da3cb1a9c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Let's check the correlation with target and find out which variables are mostly correlated with the target var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtarget_correlation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_correlation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Let's check the correlation with target and find out which variables are mostly correlated with the target var\n",
    "target_correlation = pd.DataFrame(training_data.corr()).iloc[1:,0]\n",
    "\n",
    "target_correlation.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable with the higher positive correlation (pearson) has a value of 0.06731 (var_6) so we do not have many hopes for linear models.\n",
    "Var_81 is the most negative one with a value -0.080917. Mostly the correlation between variables range between these values higher than -0.1 and lower that 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers from variables using interquartile range\n",
    "Q1 = training_data.iloc[:,2:].quantile(0.25)\n",
    "Q3 = training_data.iloc[:,2:].quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "\n",
    "train_data_outliers = training_data[~((training_data.iloc[:,2:] < (Q1 - 1.5 * IQR)) |(training_data.iloc[:,2:] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(\"Number of outliers removed was : {} rows\".format(len(training_data)-len(train_data_outliers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Being a binary classification problem, class imbalance might be a problem. Let's check it:\n",
    "print(\"% of target variable is {} with {} positive cases.\".format(train_data_outliers.target.sum()/len(train_data_outliers),train_data_outliers.target.sum()))\n",
    "\n",
    "#Only 9.7% of the target is a positive variable. Let's undersample negative cases and construct the train and test datasets\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def construct_Train_Test(weight, full_sample):  \n",
    "    if full_sample == 1:\n",
    "        a, b, c, d = train_test_split(training_data.iloc[:,2:], training_data['target'], test_size = 0.2)\n",
    "        a = sc_X.fit_transform(a)\n",
    "        b = sc_X.transform(b) \n",
    "    elif weight == 0:\n",
    "        sample_data = training_data.sample(frac = 0.2)\n",
    "        print(\"% of positive target values on training data is {}\".format(training_data['target'].sum()/len(training_data)))\n",
    "        a, b, c, d = train_test_split(sample_data.iloc[:,2:], sample_data['target'], test_size = 0.2)\n",
    "        a = sc_X.fit_transform(a)\n",
    "        b = sc_X.transform(b)\n",
    "    else: \n",
    "        positive_cases = training_data.loc[training_data['target']==1]\n",
    "        negative_cases = training_data.loc[training_data['target']==0].sample(int(weight*len(positive_cases)))\n",
    "        full_table = positive_cases.append(negative_cases)\n",
    "        a, b, c, d = train_test_split(full_table.iloc[:,2:], full_table['target'], test_size = 0.2)\n",
    "        a = sc_X.fit_transform(a)\n",
    "        b = sc_X.transform(b)\n",
    "    return a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Random Forest\n",
    "def Random_Forest_Baseline():\n",
    "    baseline = RandomForestClassifier()\n",
    "    return baseline\n",
    "\n",
    "#Neural Network Parameters\n",
    "def Neural_Network_Sigmoid(activ):\n",
    "    #Initializing the Neural Network \n",
    "    ann = Sequential()\n",
    "    #Adding the input layer and the two initial hidden layers \n",
    "    ann.add(Dense(output_dim = 128, activation = activ, input_dim = 200))\n",
    "    ann.add(Dense(output_dim = 128, activation = activ))\n",
    "    ann.add(Dense(output_dim = 128, activation = activ))\n",
    "    #Adding the Output Layer\n",
    "    ann.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "    #Compilar a Rede Neuronal - Stochastic Gradient Descent\n",
    "    ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=[auc])\n",
    "    return ann\n",
    "\n",
    "#Neural Network Parameters\n",
    "def Neural_Network_Relu(activ):\n",
    "    #Initializing the Neural Network \n",
    "    ann = Sequential()\n",
    "    #Adding the input layer and the two initial hidden layers \n",
    "    ann.add(Dense(output_dim = 128, activation = activ, input_dim = 200))\n",
    "    ann.add(Dense(output_dim = 128, activation = activ))\n",
    "    ann.add(Dense(output_dim = 128, activation = activ))\n",
    "    #Adding the Output Layer\n",
    "    ann.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "    #Compilar a Rede Neuronal - Stochastic Gradient Descent\n",
    "    ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=[auc])\n",
    "    return ann\n",
    "\n",
    "#Neural Network Parameters - With Regularization\n",
    "def Neural_Network_Relu_Regul(activ):\n",
    "    #Initializing the Neural Network \n",
    "    ann = Sequential()\n",
    "    #Adding the input layer and the two initial hidden layers \n",
    "    ann.add(Dense(output_dim = 124, activation = activ, input_dim = 200))\n",
    "    ann.add(Dropout(0.5))\n",
    "    ann.add(Dense(output_dim = 124, activation = activ))\n",
    "    ann.add(Dropout(0.5))\n",
    "    ann.add(Dense(output_dim = 24, activation = activ))\n",
    "    ann.add(Dropout(0.5))\n",
    "    ann.add(Dense(output_dim = 24, activation = activ ,\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "    #Adding the Output Layer\n",
    "    ann.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "    #Compilar a Rede Neuronal - Stochastic Gradient Descent\n",
    "    ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=[auc])\n",
    "    return ann\n",
    "\n",
    "#Random Forest Randomized Search Function Definition\n",
    "def Random_Forest_Model_Random_Search():\n",
    "    #Initializing the Neural Network \n",
    "    # Number of parameters for randomized grid search\n",
    "    rf = RandomForestClassifier()\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 400, num = 5)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [5,50,500]\n",
    "    min_samples_leaf = [4,10,100]\n",
    "    bootstrap = [True, False]\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=20, random_state=100, n_jobs = -1, scoring='roc_auc')\n",
    "    return rf_random\n",
    "\n",
    "\n",
    "#Random Forest with Best Parameters selected by randomized search\n",
    "def Random_Forest_Model_Params(params):\n",
    "        rf = RandomForestClassifier(**params)\n",
    "        return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Class Implementation\n",
    "models = {}\n",
    "\n",
    "class model():\n",
    "    \n",
    "    def __init__(self, model_type, model_object, weight_train, full_sample):\n",
    "        self.model_type = model_type\n",
    "        self.model_object = model_object\n",
    "        self.weight_train = weight_train\n",
    "        self.full_sample = full_sample\n",
    "    \n",
    "    def get_train_test(self):\n",
    "        a, b, c, d = construct_Train_Test(self.weight_train, self.full_sample)\n",
    "        return a,b,c,d\n",
    "    \n",
    "    def train_model(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        if self.model_type == 'Neural Network Sigmoid' or self.model_type == \"Neural Network Relu\":\n",
    "            X_train, X_test, y_train, y_test = construct_Train_Test(self.weight_train, self.full_sample)\n",
    "            self.history = self.model_object.fit(X_train, y_train, epochs=epochs, validation_split=0.2, shuffle=True)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = construct_Train_Test(self.weight_train, self.full_sample)\n",
    "            print(\"Size of X_train object is {}\".format(X_train.shape))\n",
    "            self.model_object.fit(X_train, y_train)\n",
    "            self.history = None\n",
    "        return self.history\n",
    "\n",
    "    def get_auc(self):\n",
    "        if self.model_type == 'Neural Network Sigmoid':\n",
    "            y_pred = self.model_object.predict(sc_X.transform(training_data.iloc[:,2:]))\n",
    "            model_key = 'For Model Type {}, number of epochs {} and weight of {} for each positive value on training'.format(self.model_type, self.weight_train) \n",
    "            models[model_key] = roc_auc_score(training_data.iloc[:,1], y_pred)\n",
    "        elif self.model_type == 'Neural Network Relu':\n",
    "            y_pred = self.model_object.predict(sc_X.transform(training_data.iloc[:,2:]))\n",
    "            model_key = 'For Model Type {}, number of epochs {} and weight of {} for each positive value on training'.format(self.model_type , self.epochs, self.weight_train) \n",
    "            models[model_key] = roc_auc_score(training_data.iloc[:,1], y_pred)\n",
    "            \n",
    "    def plot_learning_curve(self):\n",
    "        if self.model_type == 'Neural Network Sigmoid' or self.model_type == \"Neural Network Relu\":\n",
    "            plt.plot(np.arange(0,self.epochs), self.history.history['loss'], '--', color=\"#111111\",  label=\"Training Accuracy - Relu Activation\")\n",
    "            plt.plot(np.arange(0,self.epochs), self.history.history['val_loss'], color=\"#111111\", label=\"Cross-validation Accuracy - Activation\")\n",
    "            # Create plot\n",
    "            plt.title(\"Learning Curve\")\n",
    "            plt.xlabel(\"Nb. Epochs\"), plt.ylabel(\"Binary Cross Entropy\"), plt.legend(loc=\"best\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Model: Neural Network with Rectified Linear Unit activation function. - Train it with the entire sample first - plot Learning Curve to check bias and variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=200, units=128)`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b2d62da32766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train Neural Network with Relu Activation function on full training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mannr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network_Relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0martificial_nn_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Neural Network Relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneural_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martificial_nn_relu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0martificial_nn_relu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-13e8b4f4d085>\u001b[0m in \u001b[0;36mNeural_Network_Relu\u001b[0;34m(activ)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#Compilar a Rede Neuronal - Stochastic Gradient Descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'auc' is not defined"
     ]
    }
   ],
   "source": [
    "#Train Neural Network with Relu Activation function on full training data\n",
    "annr = Neural_Network_Relu('relu')\n",
    "artificial_nn_relu = model('Neural Network Relu', annr, 0, 1)\n",
    "neural_model = artificial_nn_relu.train_model(100) \n",
    "artificial_nn_relu.plot_learning_curve()\n",
    "artificial_nn_relu.get_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the loss function, we can see that we are overfitting our hypothesis. We will add regularizations to the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Neural Network with Relu Activation function on full training data\n",
    "annr_reg = Neural_Network_Relu_Regul('relu')\n",
    "artificial_nn_relu_reg = model('Neural Network Relu', annr_reg, 1, 0)\n",
    "neural_model = artificial_nn_relu_reg.train_model(100) \n",
    "artificial_nn_relu_reg.plot_learning_curve()\n",
    "artificial_nn_relu_reg.get_auc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train same Neural Network with relu on an \"artificial\" data set with balanced classes - Looping through several weights\n",
    "artificial_nn_relu_reg = model('Neural Network Relu', annr_reg, i, 0)\n",
    "neural_model = artificial_nn_relu_reg.train_model(100) \n",
    "artificial_nn_relu_reg.plot_learning_curve()\n",
    "artificial_nn_relu_reg.get_auc()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50 epochs, Relu activation function seems to work best to this problem. We will train the model with 100 epochs just to check the validity of this statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Neural Network with Relu Activation function on full training data - 100 epochs\n",
    "annr = Neural_Network_Relu('relu')\n",
    "artificial_nn_relu = model('Neural Network Relu', annr, 0, 1)\n",
    "neural_model_history = artificial_nn_relu.train_model(100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,100), neural_model_history.history['auc'], '--', color=\"#111111\",  label=\"Training AUC - Relu Activation\")\n",
    "plt.plot(np.arange(0,100), neural_model_history.history['val_auc'], color=\"#111111\", label=\"Cross-validation AUC - Activation\")\n",
    "plt.plot(np.arange(0,100), neural_model_s.history['auc'], '--', color='red',  label=\"Training AUC - Sigmoid Activation\")\n",
    "plt.plot(np.arange(0,100), neural_model_s.history['val_auc'], color='red', label=\"Cross-validation AUC - Sigmoid Activation\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Nb. Epochs\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu and Sigmoid activation functions start to converge but still have a difference in AUC. And the learning curve stabilizes with lesser epochs for Relu function so we will choose that as baseline model.\n",
    "Let's check if balancing the dataset (remember that we only have 1 in 10 cases of positive target) can boost the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Neural Network with Relu Activation function on full training data - 100 epochs - 3 positive cases for each negative case on the dataset\n",
    "annr_balanced = Neural_Network_Relu('relu')\n",
    "artificial_nn_relu_balanced = model('Neural Network Relu', annr_balanced, 3, 0)\n",
    "neural_model_balanced = artificial_nn_relu_balanced.train_model(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,100), neural_model_history.history['auc'], '--', color=\"#111111\",  label=\"Training AUC - Relu Activation - Full Sample\")\n",
    "plt.plot(np.arange(0,100), neural_model_history.history['val_auc'], color=\"#111111\", label=\"Cross-validation AUC - Relu Activation - Full Sample\")\n",
    "plt.plot(np.arange(0,100), neural_model_balanced.history['auc'], '--', color='red',  label=\"Training AUC - Relu Activation - Balanced 3/1\")\n",
    "plt.plot(np.arange(0,100), neural_model_balanced.history['val_auc'], color='red', label=\"Cross-validation AUC - Relu Activation - Balanced 3/1\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Nb. Epochs\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Very promising.. \n",
    "Not so fast. We are calculating the AUC on a \"balanced\" data set and this does not mean that the scoring on the real data will work as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating AUC on dictionary for each model\n",
    "artificial_nn_relu.get_auc()\n",
    "artificial_nn_relu_balanced.get_auc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight 0 means full sample. There is not a big difference between AUC's for both methods but full sample wins.\n",
    "Let's see how a Random Forest might fit the data - We will use a randomized search to find the best parameters.\n",
    "To find them we use a sample of 20% of the full data set as training a lot of forests on 1 millions X 200 features data is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest - We have trained a random forest randomized search and the parameters that we have got were {'n_estimators':400, 'min_samples_split':10,'min_samples_leaf':10, 'max_features':'sqrt', 'max_depth':60}\n",
    "best_params = {'n_estimators':400, 'min_samples_split':10,'min_samples_leaf':10, 'max_features':'sqrt', 'max_depth':60}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = Random_Forest_Model_Params(best_params)\n",
    "X_train, X_test, y_train, y_test = construct_Train_Test(0, 1)\n",
    "train_sizes, train_scores, test_scores = learning_curve(rf_best_params, X_train, y_train,\n",
    "                                                        cv=2,\n",
    "                                                            scoring='roc_auc',\n",
    "                                                        n_jobs=-1, \n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 30),\n",
    "                                                        verbose = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"red\",  label=\"Training ROC AUC\")\n",
    "plt.plot(train_sizes, test_mean, color=\"black\", label=\"Cross-validation ROC AUC\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awful overfitting! Made a rookie error in the randomized search and possibly will have to train a random forest with different parameters. We will modify min_samples_split, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest - We have trained a random forest randomized search and the parameters that we have got were {'n_estimators':400, 'min_samples_split':10,'min_samples_leaf':10, 'max_features':'sqrt', 'max_depth':60}\n",
    "best_params_alternative = {'n_estimators':100, 'min_samples_split':1000,'min_samples_leaf':1000, 'max_features':'sqrt', 'max_depth':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params_alternative = Random_Forest_Model_Params(best_params_alternative)\n",
    "X_train, X_test, y_train, y_test = construct_Train_Test(0, 1)\n",
    "train_sizes, train_scores, test_scores = learning_curve(rf_best_params_alternative, X_train, y_train,\n",
    "                                                        cv=2,\n",
    "                                                            scoring='roc_auc',\n",
    "                                                        n_jobs=-1, \n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 10),\n",
    "                                                        verbose = 10)                    \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"red\",  label=\"Training ROC AUC\")\n",
    "plt.plot(train_sizes, test_mean, color=\"black\", label=\"Cross-validation ROC AUC\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
